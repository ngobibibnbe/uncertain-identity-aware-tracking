{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ4r6kvLa7oC"
      },
      "source": [
        "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_XzjKmXa7oF"
      },
      "source": [
        "# Training a Multi-Object Tracking Model\n",
        "\n",
        "This notebook shows how to train and evaluate a multi-object tracking model.\n",
        "\n",
        "Specifically, this notebook uses [FairMOT](https://github.com/ifzhang/FairMOT), a state-of-the-art tracking model with high accuracy and fast inference speed. The model is trained on a set of still images, and is then evaluated on video footage. For more information regarding FairMOT and multi-object tracking, please visit the [FAQ](./FAQ.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9CxX11la7oF"
      },
      "source": [
        "## Initialization\n",
        "Import all the functions we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mw69EsY1jbT",
        "outputId": "cc42dd3e-0843-48d7-ae19-74d0334536a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!pip install -q condacolab\\nimport condacolab\\ncondacolab.install()'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iITV3ez11o7J",
        "outputId": "92ee22aa-5d30-4f4d-eb86-3ea6e1e504ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!conda install -c conda-forge opencv yacs lap progress\\n!pip install cython_bbox motmetrics'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!conda install -c conda-forge opencv yacs lap progress\n",
        "!pip install cython_bbox motmetrics\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uaox9Gg12hEP"
      },
      "outputs": [],
      "source": [
        "#%cd computervision-recipes/utils_cv/tracking/references/fairmot/models/networks/DCNv2\n",
        "#!sh make.sh\n",
        "#!pip install git+https://github.com/microsoft/ComputerVision.git@master#egg=utils_cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "u5awu52Sa7oG",
        "outputId": "aabfa4c2-7ad4-4f14-d7fa-f022ed0ee4f0",
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils_cv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3330286/4036052619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Computer Vision repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munzip_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUrls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_cv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "\n",
        "from ipywidgets import Video\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Computer Vision repository\n",
        "sys.path.append(\"../../\")\n",
        "from utils_cv.common.data import data_path, unzip_url\n",
        "from utils_cv.common.gpu import is_windows, which_processor\n",
        "from utils_cv.tracking.data import Urls\n",
        "from utils_cv.tracking.dataset import TrackingDataset\n",
        "from utils_cv.tracking.model import TrackingLearner\n",
        "from utils_cv.tracking.plot import plot_single_frame, play_video, write_video\n",
        "\n",
        "# Change matplotlib backend so that plots are shown for windows\n",
        "if is_windows():\n",
        "    plt.switch_backend(\"TkAgg\")\n",
        "\n",
        "print(f\"TorchVision: {torchvision.__version__}\")\n",
        "which_processor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqr8gAbA0lOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VKfEciVa7oI"
      },
      "source": [
        "The above torchvision command displays your machine's GPUs (if it has any) and the compute that `torch/torchvision` is using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSsYlklba7oJ"
      },
      "outputs": [],
      "source": [
        "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhYQcguca7oK"
      },
      "source": [
        "Next, set some training and inference parameters, as well as the data input parameters. Better accuracy can typically be achieved by increasing the number of training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1NIf23_a7oL",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.0005\n",
        "BATCH_SIZE = 4\n",
        "MODEL_PATH = \"./models/all_dla34.pth\"  # the path of the pretrained model to finetune/train\n",
        "\n",
        "# Inference parameters\n",
        "CONF_THRES = 0.3\n",
        "\n",
        "# Data Location\n",
        "TRAIN_DATA_PATH = unzip_url(Urls.cans_path, exist_ok=True)\n",
        "EVAL_DATA_PATH = unzip_url(Urls.carcans_annotations_path, exist_ok=True)\n",
        "\n",
        "# Train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using torch device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkkQtacSa7oM"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Please follow the setup instructions in the [README.md](https://github.com/microsoft/computervision-recipes/blob/master/scenarios/tracking/README.md) to make sure all required libraries are installed.\n",
        "\n",
        "In addition, to be able to run this notebook, the baseline FairMOT model needs to be downloaded from [here](https://drive.google.com/file/d/1udpOPum8fJdoEQm6n0jsIgMMViOMFinu/view) and saved to the `./models` folder as `all_dla.pth`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDGyWt51a7oM"
      },
      "source": [
        "## Prepare Training Dataset\n",
        "\n",
        "This section will show how to use a small training dataset to finetune a pre-trained model. The dataset consists of 12 images of cans across four classes `{coke, gingerale, espresso, coldbrew}` and with differing backgrounds.\n",
        "\n",
        "Note that we use different cans for training, so that the re-id component in the FairMOT tracker can learn to distinguish different type of cans from one-another. During inference time, this will enable the tracker to distinguish between cans it had not seen during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx_4tCj3a7oN"
      },
      "outputs": [],
      "source": [
        "os.listdir(TRAIN_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce465wVQa7oQ"
      },
      "source": [
        "Within the data folder there are two different subfolders:\n",
        "- `/images/`\n",
        "- `/annotations/`\n",
        "\n",
        "This format, one folder for images and one folder for annotations, is common for object detection and object tracking. In fact, the annotation format (Pascal VOC) is identical to the annotation format used for object detection - see the [01_training_introduction.ipynb](../detection/01_training_introduction.ipynb) notebook for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9VhvRlIa7oQ"
      },
      "source": [
        "## Load Training Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNNm3wsVa7oR"
      },
      "source": [
        "To load the data, we use the `TrackingDataset` class. This object knows how to read images and annotations consistent with the  format specified above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfRTBfUJa7oR"
      },
      "outputs": [],
      "source": [
        "data_train = TrackingDataset(TRAIN_DATA_PATH, batch_size=BATCH_SIZE)\n",
        "print(\"Found {} training images.\".format(len(data_train.im_filenames)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IENCqt5Oa7oR"
      },
      "outputs": [],
      "source": [
        "data_train.show_ims()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db_bZjaUa7oS"
      },
      "source": [
        "## Finetune a Pretrained Model\n",
        "\n",
        "For the TrackingLearner, we use FairMOT's baseline tracking model. FairMOT's baseline tracking model is pre-trained on pedestrian datasets, such as the [MOT challenge datasets](https://motchallenge.net/). Therefore, it does not yet know how to detect cans.\n",
        "\n",
        "When we initialize the TrackingLearner, we can pass in the training dataset and the path to the baseline\n",
        "model which by default is `./models/all_dla.pth`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcGKk0Gia7oS"
      },
      "outputs": [],
      "source": [
        "tracker = TrackingLearner(data_train, MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN3AmomGa7oS"
      },
      "source": [
        "To run the training, we call the `fit` method in the tracker object. Note that we reduce the learning rate by a factor of 10 after 75% of the epochs to improve convergence to a good minima of the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5jVhF8Fa7oT"
      },
      "outputs": [],
      "source": [
        "tracker.fit(num_epochs=EPOCHS, lr=LEARNING_RATE, lr_step = round(0.75*EPOCHS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrM5CFrga7oT"
      },
      "source": [
        "The function below visualizes the training losses after each epoch, and shows how the model improves over time. With appropriate values for `num_epochs` and `lr` this loss-curve should converge towards zero. The loss-curve for our training is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgn2DPxda7oT"
      },
      "outputs": [],
      "source": [
        "tracker.plot_training_losses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfGkepk4a7oU"
      },
      "source": [
        "# Predict and Evaluate Tracking\n",
        "To validate the trained model, we run it on an evaluation dataset and compare the predicted tracking results with the dataset's ground-truth annotations.\n",
        "\n",
        "For that, we annotated each frame of a one second long video sequence called `car_cans_1s.mp4`. For more details on how to prepare the annotation and evaluation dataset please see the [FAQ](./FAQ.md)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok5VZEIna7oU"
      },
      "outputs": [],
      "source": [
        "eval_video_path = osp.join(EVAL_DATA_PATH, \"car_cans_1s.mp4\")\n",
        "#Video.from_file(eval_video_path)   # uncomment this line to play the video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aRBgR4Ga7oU"
      },
      "source": [
        "This shows a single frame from around the middle of the evaluation video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0QPpwKaa7oU"
      },
      "outputs": [],
      "source": [
        "plot_single_frame(eval_video_path, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3zS_Ayea7oU"
      },
      "source": [
        "## Predict\n",
        "\n",
        "Now, we can run the `predict` function on our evaluation dataset. Note that there are several parameters that can be tweaked to improve the tracking performance and inference speed, including `conf_thres` or `track_buffer`. Please see the  [FAQ](./FAQ.md) for more details.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqmgpTy5a7oU",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "eval_results = tracker.predict(\n",
        "    EVAL_DATA_PATH, conf_thres=CONF_THRES,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt2UaTEia7oU"
      },
      "source": [
        "The call to `predict` returns the dictionary `eval_results` where each key is the frame number, and the value is a list of `TrackingBbox` objects that represent the tracking information of each object detected. For example, when we print out the tracking results from the last frame (frame 30), we can see two objects being tracked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7xYEFE9a7oU"
      },
      "outputs": [],
      "source": [
        "print(\"Last frame...tracking result:\", eval_results[max(eval_results.keys())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iOB828Pa7oV"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "To obtain quantitiative evaluation metrics, we can simply pass on our `tracking_results` dictionary to the `evaluate` method in the tracker object. This outputs common MOT metrics such as IDF1 or MOTA. Please refer to the [FAQ](./FAQ.md) for more details on MOT metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w-g1ROOa7oV",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "eval_metrics = tracker.evaluate(eval_results, EVAL_DATA_PATH)\n",
        "print(eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrTfQnQa7oV"
      },
      "source": [
        "## Visualize results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4_YmIIma7oV"
      },
      "source": [
        "We can visualize the tracking results by overlaying the bounding boxes and ids of the tracked objects onto the video and writing it to the following file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNZf7vJpa7oW"
      },
      "outputs": [],
      "source": [
        "write_video(eval_results, eval_video_path, \"results_eval.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T1NI-26a7oW"
      },
      "source": [
        "The following cell extracts and displays certain frames from this video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66f3TH1Ra7oW",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for frame_i in [1, int(len(eval_results) / 2), len(eval_results) - 3]:\n",
        "    im = plot_single_frame(eval_video_path, frame_i, eval_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQSnK_4a7oW"
      },
      "source": [
        "In addition, we can play the video here in the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQkGwV4Ja7oX",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "play_video(eval_results, eval_video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iRcp51ha7ot"
      },
      "source": [
        "## Save the trained model\n",
        "If satified with the results from evalutation, we can save this finetuned model to disk for later use.\n",
        "```\n",
        "tracker.save(TRAINED_MODEL_PATH)\n",
        "```\n",
        "\n",
        "To load the model and track objects in a new video these commands can be used\n",
        "```\n",
        "tracker = TrackingLearner(None, TRAINED_MODEL_PATH)\n",
        "test_results = tracker.predict(\n",
        "    path_to_video, conf_thres=CONF_THRES, track_buffer=TRACK_BUFFER,\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fairmot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "279.417px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
